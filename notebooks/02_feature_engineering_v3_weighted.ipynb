{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline V3 - WEIGHTED ACTIONS\n",
    "\n",
    "## –ö–ª—é—á–µ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ:\n",
    "**–†–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –¥–µ–π—Å—Ç–≤–∏–π –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–π –≤–µ—Å –ø—Ä–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏!**\n",
    "\n",
    "### Action Weights:\n",
    "- **order** (3): 5.0 - —Å–∞–º—ã–π —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª (–ø–æ–∫—É–ø–∫–∞)\n",
    "- **to_cart** (5): 3.0 - —Å–∏–ª—å–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ\n",
    "- **favorite** (2): 2.5 - —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–µ—Å\n",
    "- **search**: 2.0 - —è–≤–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ (—á—Ç–æ –∏—â–µ—Ç)\n",
    "- **click** (1): 1.0 - –±–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä–µ—Å\n",
    "\n",
    "### –ù–æ–≤—ã–µ —Ñ–∏—á–∏:\n",
    "- `weighted_total_actions` - —Å—É–º–º–∞ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π\n",
    "- `weighted_engagement_score` - –æ–±—â–∏–π engagement —Å –≤–µ—Å–∞–º–∏\n",
    "- `action_diversity` - —ç–Ω—Ç—Ä–æ–ø–∏—è —Ç–∏–ø–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π\n",
    "- `high_intent_ratio` - –¥–æ–ª—è high-intent –¥–µ–π—Å—Ç–≤–∏–π (order, cart)\n",
    "- `recent_weighted_activity` - –Ω–µ–¥–∞–≤–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å –≤–µ—Å–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WEIGHTED FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "Action Weights:\n",
      "  click: 1.0\n",
      "  favorite: 2.5\n",
      "  order: 5.0\n",
      "  to_cart: 3.0\n",
      "  search: 2.0\n",
      "\n",
      "Train: 2024-03-01 - 2024-06-30\n",
      "Val: 2024-07-01 - 2024-07-31\n",
      "Test: 2024-08-01\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../docs'\n",
    "\n",
    "TRAIN_START_DATE = pd.Timestamp('2024-03-01')\n",
    "TRAIN_END_DATE = pd.Timestamp('2024-06-30')\n",
    "VAL_START_DATE = pd.Timestamp('2024-07-01')\n",
    "VAL_END_DATE = pd.Timestamp('2024-07-31')\n",
    "TEST_START_DATE = pd.Timestamp('2024-08-01')\n",
    "NUM_PERIODS = 4\n",
    "\n",
    "\n",
    "ACTION_WEIGHTS = {\n",
    "    1: 1.0,   # click - –±–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä–µ—Å\n",
    "    2: 2.5,   # favorite - —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–µ—Å\n",
    "    3: 5.0,   # order - –°–ê–ú–´–ô –í–ê–ñ–ù–´–ô (–ø–æ–∫—É–ø–∫–∞)\n",
    "    5: 3.0,   # to_cart - —Å–∏–ª—å–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ\n",
    "}\n",
    "\n",
    "SEARCH_WEIGHT = 2.0  # –ü–æ–∏—Å–∫ = —è–≤–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WEIGHTED FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAction Weights:\")\n",
    "for action_id, weight in ACTION_WEIGHTS.items():\n",
    "    action_name = {1: 'click', 2: 'favorite', 3: 'order', 5: 'to_cart'}[action_id]\n",
    "    print(f\"  {action_name}: {weight}\")\n",
    "print(f\"  search: {SEARCH_WEIGHT}\")\n",
    "\n",
    "print(f\"\\nTrain: {TRAIN_START_DATE.date()} - {TRAIN_END_DATE.date()}\")\n",
    "print(f\"Val: {VAL_START_DATE.date()} - {VAL_END_DATE.date()}\")\n",
    "print(f\"Test: {TEST_START_DATE.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading actions_history...\n",
      "Found 53 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [00:03<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions: (182001544, 7)\n",
      "Date range: 2011-05-28 00:26:26 - 2024-07-31 23:59:58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load actions \n",
    "print(\"Loading actions_history...\")\n",
    "actions_files = sorted(glob.glob(os.path.join(DATA_PATH, 'actions_history', '*.parquet')))\n",
    "print(f\"Found {len(actions_files)} files\")\n",
    "\n",
    "actions_list = []\n",
    "for file in tqdm(actions_files, desc=\"Loading\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    # ADD WEIGHT COLUMN\n",
    "    df['action_weight'] = df['action_type_id'].map(ACTION_WEIGHTS)\n",
    "    actions_list.append(df)\n",
    "\n",
    "actions_history = pd.concat(actions_list, ignore_index=True)\n",
    "print(f\"\\nActions: {actions_history.shape}\")\n",
    "print(f\"Date range: {actions_history['timestamp'].min()} - {actions_history['timestamp'].max()}\")\n",
    "\n",
    "del actions_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading search_history...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searches: (78160845, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load searches\n",
    "print(\"Loading search_history...\")\n",
    "search_files = sorted(glob.glob(os.path.join(DATA_PATH, 'search_history', '*.parquet')))\n",
    "\n",
    "search_list = []\n",
    "for file in tqdm(search_files, desc=\"Loading\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    # ADD SEARCH WEIGHT\n",
    "    df['action_weight'] = SEARCH_WEIGHT\n",
    "    search_list.append(df)\n",
    "\n",
    "search_history = pd.concat(search_list, ignore_index=True)\n",
    "print(f\"\\nSearches: {search_history.shape}\")\n",
    "\n",
    "del search_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: (238443, 8)\n",
      "Test users: (2068424, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load products and test users\n",
    "product_information = pd.read_csv(os.path.join(DATA_PATH, 'product_information.csv'))\n",
    "test_users = pd.read_csv(os.path.join(DATA_PATH, 'test_users.csv'))\n",
    "\n",
    "print(f\"Products: {product_information.shape}\")\n",
    "print(f\"Test users: {test_users.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 1,835,147\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    1200425\n",
      "1     634722\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive ratio: 34.59%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation target\n",
    "val_actions = actions_history[\n",
    "    (actions_history['timestamp'] >= VAL_START_DATE) &\n",
    "    (actions_history['timestamp'] <= VAL_END_DATE)\n",
    "].copy()\n",
    "\n",
    "val_target = (\n",
    "    val_actions\n",
    "    .assign(has_order=(val_actions['action_type_id'] == 3).astype(int))\n",
    "    .groupby('user_id', as_index=False)\n",
    "    .agg(target=('has_order', 'max'))\n",
    ")\n",
    "\n",
    "print(f\"Total users: {val_target.shape[0]:,}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(val_target['target'].value_counts())\n",
    "print(f\"\\nPositive ratio: {val_target['target'].mean():.2%}\")\n",
    "\n",
    "del val_actions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weighted Feature Generation\n",
    "\n",
    "### 4.1 Basic RFM Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_rfm_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate RFM features with WEIGHTS for different action types.\n",
    "    \n",
    "    NEW FEATURES:\n",
    "    - weighted_total_actions: —Å—É–º–º–∞ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π\n",
    "    - weighted_products: —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã —Å —É—á–µ—Ç–æ–º –≤–µ—Å–∞ –¥–µ–π—Å—Ç–≤–∏–π\n",
    "    - high_intent_ratio: –¥–æ–ª—è high-intent –¥–µ–π—Å—Ç–≤–∏–π (order+cart)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Weighted RFM Features ===\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    period_actions = period_actions.merge(\n",
    "        product_information[['product_id', 'discount_price']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    actions_map = {1: 'click', 2: 'favorite', 3: 'order', 5: 'to_cart'}\n",
    "    \n",
    "    # Per-action features (with weights)\n",
    "    for action_id, suffix in actions_map.items():\n",
    "        print(f\"  {suffix}...\")\n",
    "        \n",
    "        action_data = period_actions[period_actions['action_type_id'] == action_id].copy()\n",
    "        \n",
    "        if len(action_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        weight = ACTION_WEIGHTS[action_id]\n",
    "        \n",
    "        aggs = action_data.groupby('user_id').agg(\n",
    "            **{\n",
    "                f'num_products_{suffix}': ('product_id', 'count'),\n",
    "                f'num_unique_products_{suffix}': ('product_id', 'nunique'),\n",
    "                f'weighted_count_{suffix}': ('action_weight', 'sum'),  \n",
    "                f'sum_discount_price_{suffix}': ('discount_price', 'sum'),\n",
    "                f'max_discount_price_{suffix}': ('discount_price', 'max'),\n",
    "                f'last_{suffix}_time': ('timestamp', 'max'),\n",
    "                f'first_{suffix}_time': ('timestamp', 'min'),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Recency\n",
    "        ref = end_date + timedelta(days=1)\n",
    "        aggs[f'days_since_last_{suffix}'] = (ref - aggs[f'last_{suffix}_time']).dt.days\n",
    "        aggs[f'days_since_first_{suffix}'] = (ref - aggs[f'first_{suffix}_time']).dt.days\n",
    "        \n",
    "        # Weighted recency (–Ω–µ–¥–∞–≤–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≤–∞–∂–Ω–µ–µ)\n",
    "        aggs[f'recency_weighted_{suffix}'] = (\n",
    "            weight / (aggs[f'days_since_last_{suffix}'] + 1)\n",
    "        )\n",
    "        \n",
    "        aggs = aggs.drop(columns=[f'last_{suffix}_time', f'first_{suffix}_time'])\n",
    "        df = df.merge(aggs, on='user_id', how='left')\n",
    "    \n",
    "    # Search features \n",
    "    print(\"  search...\")\n",
    "    period_searches = search_history[\n",
    "        (search_history['timestamp'] >= start_date) &\n",
    "        (search_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(period_searches) > 0:\n",
    "        search_aggs = period_searches.groupby('user_id').agg(\n",
    "            num_search=('search_query', 'count'),\n",
    "            weighted_search_count=('action_weight', 'sum'),  # NEW\n",
    "            last_search_time=('timestamp', 'max'),\n",
    "            first_search_time=('timestamp', 'min'),\n",
    "        ).reset_index()\n",
    "        \n",
    "        ref = end_date + timedelta(days=1)\n",
    "        search_aggs['days_since_last_search'] = (ref - search_aggs['last_search_time']).dt.days\n",
    "        search_aggs['days_since_first_search'] = (ref - search_aggs['first_search_time']).dt.days\n",
    "        search_aggs['recency_weighted_search'] = (\n",
    "            SEARCH_WEIGHT / (search_aggs['days_since_last_search'] + 1)\n",
    "        )\n",
    "        \n",
    "        search_aggs = search_aggs.drop(columns=['last_search_time', 'first_search_time'])\n",
    "        df = df.merge(search_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # GLOBAL WEIGHTED FEATURES\n",
    "    print(\"  global weighted...\")\n",
    "    \n",
    "    global_aggs = period_actions.groupby('user_id').agg(\n",
    "        total_actions=('product_id', 'count'),\n",
    "        weighted_total_actions=('action_weight', 'sum'),  \n",
    "        high_intent_count=('action_type_id', lambda x: ((x == 3) | (x == 5)).sum()),  # orders + cart\n",
    "    ).reset_index()\n",
    "    \n",
    "    # High intent ratio\n",
    "    global_aggs['high_intent_ratio'] = (\n",
    "        global_aggs['high_intent_count'] / global_aggs['total_actions']\n",
    "    )\n",
    "    \n",
    "    # Action diversity (entropy)\n",
    "    action_diversity = (\n",
    "        period_actions\n",
    "        .groupby(['user_id', 'action_type_id'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    \n",
    "    # Calculate entropy\n",
    "    action_diversity['action_diversity'] = action_diversity.apply(\n",
    "        lambda row: entropy(row + 1e-10), axis=1\n",
    "    )\n",
    "    \n",
    "    global_aggs = global_aggs.merge(\n",
    "        action_diversity[['action_diversity']].reset_index(),\n",
    "        on='user_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    df = df.merge(global_aggs, on='user_id', how='left')\n",
    "    \n",
    "    new_count = len(df.columns) - len(user_df.columns)\n",
    "    print(f\"  Generated {new_count} weighted features\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Temporal patterns (same as before)\"\"\"\n",
    "    print(\"\\n=== Temporal Features ===\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    period_actions['day_of_week'] = period_actions['timestamp'].dt.dayofweek\n",
    "    period_actions['hour'] = period_actions['timestamp'].dt.hour\n",
    "    period_actions['date'] = period_actions['timestamp'].dt.date\n",
    "    \n",
    "    actions_map = {1: 'click', 2: 'favorite', 3: 'order', 5: 'to_cart'}\n",
    "    \n",
    "    for action_id, suffix in actions_map.items():\n",
    "        action_data = period_actions[period_actions['action_type_id'] == action_id]\n",
    "        \n",
    "        if len(action_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        temporal = action_data.groupby('user_id').agg(\n",
    "            **{\n",
    "                f'favorite_day_of_week_{suffix}': ('day_of_week', 'median'),\n",
    "                f'avg_hour_{suffix}': ('hour', 'median'),\n",
    "                f'num_unique_days_{suffix}': ('date', 'nunique'),\n",
    "                f'first_time_{suffix}': ('timestamp', 'min'),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        temporal[f'is_new_user_{suffix}'] = (\n",
    "            temporal[f'first_time_{suffix}'] >= pd.Timestamp('2024-06-01')\n",
    "        ).astype(int)\n",
    "        \n",
    "        temporal = temporal.drop(columns=[f'first_time_{suffix}'])\n",
    "        df = df.merge(temporal, on='user_id', how='left')\n",
    "    \n",
    "    # Lifetime\n",
    "    for suffix in ['click', 'favorite', 'order', 'to_cart']:\n",
    "        first_col = f'days_since_first_{suffix}'\n",
    "        last_col = f'days_since_last_{suffix}'\n",
    "        if first_col in df.columns and last_col in df.columns:\n",
    "            df[f'lifetime_{suffix}'] = df[first_col] - df[last_col]\n",
    "    \n",
    "    print(\"  Generated temporal features\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conversion Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversion_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Conversion rates (same as before)\"\"\"\n",
    "    print(\"\\n=== Conversion Features ===\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    for suffix in ['click', 'favorite', 'to_cart']:\n",
    "        num_col = f'num_products_{suffix}'\n",
    "        if num_col in df.columns and 'num_products_order' in df.columns:\n",
    "            df[f'{suffix}_to_order_conversion'] = (\n",
    "                df['num_products_order'] / df[num_col].replace(0, np.nan)\n",
    "            )\n",
    "    \n",
    "    if 'num_search' in df.columns and 'num_products_order' in df.columns:\n",
    "        df['searches_to_order_ratio'] = (\n",
    "            df['num_search'] / df['num_products_order'].replace(0, np.nan)\n",
    "        )\n",
    "    \n",
    "    for suffix in ['click', 'favorite', 'to_cart', 'order']:\n",
    "        num_col = f'num_unique_products_{suffix}'\n",
    "        days_col = f'num_unique_days_{suffix}'\n",
    "        if num_col in df.columns and days_col in df.columns:\n",
    "            df[f'{suffix}_per_day'] = df[num_col] / df[days_col].replace(0, np.nan)\n",
    "    \n",
    "    print(\"  Generated conversion features\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advanced_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Advanced behavioral features \"\"\"\n",
    "    print(\"\\n=== Advanced Features ===\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    # Discount ratio\n",
    "    order_actions = period_actions[period_actions['action_type_id'] == 3].copy()\n",
    "    \n",
    "    if len(order_actions) > 0:\n",
    "        order_actions = order_actions.merge(\n",
    "            product_information[['product_id', 'price', 'discount_price']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        order_actions['has_discount'] = (\n",
    "            order_actions['price'] > order_actions['discount_price']\n",
    "        ).astype(int)\n",
    "        \n",
    "        discount_aggs = order_actions.groupby('user_id').agg(\n",
    "            discount_purchase_ratio=('has_discount', 'mean'),\n",
    "            avg_order_price=('discount_price', 'mean')\n",
    "        ).reset_index()\n",
    "        \n",
    "        df = df.merge(discount_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # Category diversity\n",
    "    interaction_actions = period_actions[\n",
    "        period_actions['action_type_id'].isin([1, 2, 3, 5])\n",
    "    ].copy()\n",
    "    \n",
    "    if len(interaction_actions) > 0:\n",
    "        interaction_actions = interaction_actions.merge(\n",
    "            product_information[['product_id', 'category_id']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        category_aggs = interaction_actions.groupby('user_id').agg(\n",
    "            num_unique_categories=('category_id', 'nunique'),\n",
    "            total_interactions=('category_id', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        category_aggs['category_diversity'] = (\n",
    "            category_aggs['num_unique_categories'] / category_aggs['total_interactions']\n",
    "        )\n",
    "        \n",
    "        category_aggs = category_aggs.drop(columns=['total_interactions'])\n",
    "        df = df.merge(category_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # Widget diversity\n",
    "    widget_aggs = period_actions.groupby('user_id').agg(\n",
    "        num_unique_widgets=('widget_name_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    df = df.merge(widget_aggs, on='user_id', how='left')\n",
    "    \n",
    "    print(\"  Generated advanced features\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Periodic Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_periodic_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp,\n",
    "    num_periods: int = 4,\n",
    "    prefiltered_actions: pd.DataFrame = None  # NEW: accept pre-filtered data\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Generates weighted periodic features ~5x faster.\n",
    "    \n",
    "    Key optimizations:\n",
    "    1. Accept pre-filtered actions to avoid repeated filtering\n",
    "    2. Removed slow lambda for category_mode (replaced with faster approach)\n",
    "    3. Simplified aggregations\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Weighted Periodic Features (OPTIMIZED) ===\")\n",
    "    print(f\"  Periods: {num_periods} weeks + older\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    user_set = set(user_df['user_id'])\n",
    "    \n",
    "    # Use pre-filtered data if provided, otherwise filter\n",
    "    if prefiltered_actions is not None:\n",
    "        period_actions = prefiltered_actions[\n",
    "            prefiltered_actions['user_id'].isin(user_set)\n",
    "        ].copy()\n",
    "    else:\n",
    "        period_actions = actions_history[\n",
    "            (actions_history['timestamp'] >= start_date) &\n",
    "            (actions_history['timestamp'] <= end_date) &\n",
    "            (actions_history['user_id'].isin(user_set))\n",
    "        ].copy()\n",
    "    \n",
    "    if len(period_actions) == 0:\n",
    "        print(\"  No data\")\n",
    "        return df\n",
    "    \n",
    "    # Merge product info only if not already present\n",
    "    if 'category_id' not in period_actions.columns:\n",
    "        period_actions = period_actions.merge(\n",
    "            product_information[['product_id', 'category_id', 'price', 'discount_price']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # Fill missing values efficiently\n",
    "    period_actions['category_id'] = period_actions['category_id'].fillna(10000).astype('int32')\n",
    "    price_mean = period_actions['price'].mean()\n",
    "    period_actions['price'] = period_actions['price'].fillna(price_mean).astype('float32')\n",
    "    period_actions['discount_price'] = period_actions['discount_price'].fillna(price_mean).astype('float32')\n",
    "    \n",
    "    # Period assignment\n",
    "    period_actions['period'] = (\n",
    "        (end_date - period_actions['timestamp']).dt.days // 7\n",
    "    ).clip(upper=num_periods).astype('int8')\n",
    "    \n",
    "    print(\"  Aggregating (fast)...\")\n",
    "    \n",
    "    # FAST aggregation - removed slow lambda!\n",
    "    # Using only built-in aggregations which are vectorized\n",
    "    aggregated = period_actions.groupby(\n",
    "        ['user_id', 'period', 'action_type_id'], \n",
    "        as_index=False\n",
    "    ).agg(\n",
    "        num_actions=('timestamp', 'count'),  # Changed from nunique to count (faster)\n",
    "        weighted_actions=('action_weight', 'sum'),  \n",
    "        num_products=('product_id', 'nunique'),\n",
    "        count_products=('product_id', 'count'),\n",
    "        unique_widget_actions=('widget_name_id', 'nunique'),\n",
    "        num_categories=('category_id', 'nunique'),\n",
    "        price_mean=('price', 'mean'),\n",
    "        price_max=('price', 'max'),\n",
    "        discount_price_mean=('discount_price', 'mean'),\n",
    "        discount_price_max=('discount_price', 'max'),\n",
    "    )\n",
    "    \n",
    "    # Normalize period 4\n",
    "    divisor = (end_date - pd.Timedelta(f\"{num_periods*7} days\") - start_date).days\n",
    "    if divisor > 0:\n",
    "        features_norm = ['num_actions', 'weighted_actions', 'num_products', \n",
    "                        'count_products', 'unique_widget_actions', 'num_categories']\n",
    "        mask = aggregated['period'] == num_periods\n",
    "        aggregated.loc[mask, features_norm] = aggregated.loc[mask, features_norm] / divisor\n",
    "    \n",
    "    print(\"  Pivoting...\")\n",
    "    \n",
    "    # Pivot - simplified feature list (removed category_mode and timestamp_std)\n",
    "    features = [\n",
    "        'num_actions', 'weighted_actions', 'num_products', 'count_products',\n",
    "        'unique_widget_actions', 'num_categories',\n",
    "        'price_mean', 'price_max', 'discount_price_mean', 'discount_price_max'\n",
    "    ]\n",
    "    \n",
    "    aggregated_wide = aggregated.pivot_table(\n",
    "        index='user_id',\n",
    "        columns=['period', 'action_type_id'],\n",
    "        values=features,\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    aggregated_wide.columns = [\n",
    "        f\"{feat}_{period}_{action}\"\n",
    "        for feat, period, action in aggregated_wide.columns\n",
    "    ]\n",
    "    \n",
    "    aggregated_wide = aggregated_wide.reset_index()\n",
    "    df = df.merge(aggregated_wide, on='user_id', how='left')\n",
    "    \n",
    "    periodic_cols = [col for col in df.columns if col not in user_df.columns]\n",
    "    df[periodic_cols] = df[periodic_cols].fillna(0)\n",
    "    \n",
    "    new_count = len(df.columns) - len(user_df.columns)\n",
    "    print(f\"  Generated {new_count} periodic features\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING TRAINING FEATURES\n",
      "============================================================\n",
      "\n",
      "[OPTIMIZATION] Pre-filtering actions for train period...\n",
      "  Pre-filtered: 141,962,692 actions\n",
      "\n",
      "=== Weighted RFM Features ===\n",
      "  click...\n",
      "  favorite...\n",
      "  order...\n",
      "  to_cart...\n",
      "  search...\n",
      "  global weighted...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GENERATING TRAINING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== OPTIMIZATION: Pre-filter data ONCE =====\n",
    "print(\"\\n[OPTIMIZATION] Pre-filtering actions for train period...\")\n",
    "train_actions_filtered = actions_history[\n",
    "    (actions_history['timestamp'] >= TRAIN_START_DATE) &\n",
    "    (actions_history['timestamp'] <= TRAIN_END_DATE)\n",
    "].copy()\n",
    "\n",
    "# Pre-merge product info (avoids repeated merges)\n",
    "train_actions_filtered = train_actions_filtered.merge(\n",
    "    product_information[['product_id', 'category_id', 'price', 'discount_price']],\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"  Pre-filtered: {len(train_actions_filtered):,} actions\")\n",
    "\n",
    "df_train = val_target.copy()\n",
    "\n",
    "# 1. Weighted RFM\n",
    "df_train = generate_weighted_rfm_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE\n",
    ")\n",
    "\n",
    "# 2. Temporal\n",
    "df_train = generate_temporal_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE\n",
    ")\n",
    "\n",
    "# 3. Conversion\n",
    "df_train = generate_conversion_features(df_train)\n",
    "\n",
    "# 4. Advanced\n",
    "df_train = generate_advanced_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE\n",
    ")\n",
    "\n",
    "# 5. Weighted Periodic - USE PRE-FILTERED DATA\n",
    "df_train = generate_weighted_periodic_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE,\n",
    "    num_periods=4,\n",
    "    prefiltered_actions=train_actions_filtered  # OPTIMIZATION!\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "del train_actions_filtered\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TOTAL FEATURES: {len(df_train.columns) - 2}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING TEST FEATURES (WEIGHTED)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGENERATING TEST FEATURES (WEIGHTED)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df_test \u001b[38;5;241m=\u001b[39m test_users\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m df_test \u001b[38;5;241m=\u001b[39m generate_weighted_rfm_features(\n\u001b[1;32m      9\u001b[0m     df_test,\n\u001b[1;32m     10\u001b[0m     start_date\u001b[38;5;241m=\u001b[39mTRAIN_START_DATE,\n\u001b[1;32m     11\u001b[0m     end_date\u001b[38;5;241m=\u001b[39mVAL_END_DATE\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_users' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GENERATING TEST FEATURES (WEIGHTED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== OPTIMIZATION: Pre-filter data ONCE =====\n",
    "print(\"\\n[OPTIMIZATION] Pre-filtering actions for test period...\")\n",
    "test_actions_filtered = actions_history[\n",
    "    (actions_history['timestamp'] >= TRAIN_START_DATE) &\n",
    "    (actions_history['timestamp'] <= VAL_END_DATE)\n",
    "].copy()\n",
    "\n",
    "# Pre-merge product info\n",
    "test_actions_filtered = test_actions_filtered.merge(\n",
    "    product_information[['product_id', 'category_id', 'price', 'discount_price']],\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"  Pre-filtered: {len(test_actions_filtered):,} actions\")\n",
    "\n",
    "df_test = test_users.copy()\n",
    "df_test['target'] = 0\n",
    "\n",
    "df_test = generate_weighted_rfm_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE\n",
    ")\n",
    "\n",
    "df_test = generate_temporal_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE\n",
    ")\n",
    "\n",
    "df_test = generate_conversion_features(df_test)\n",
    "\n",
    "df_test = generate_advanced_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE\n",
    ")\n",
    "\n",
    "# USE PRE-FILTERED DATA\n",
    "df_test = generate_weighted_periodic_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE,\n",
    "    num_periods=4,\n",
    "    prefiltered_actions=test_actions_filtered  # OPTIMIZATION!\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "del test_actions_filtered\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TEST FEATURES: {len(df_test.columns) - 2}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns\n",
    "feature_cols = [col for col in df_train.columns if col not in ['user_id', 'target']]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Fill nulls with -1\n",
    "print(\"\\nFilling nulls with -1...\")\n",
    "df_train[feature_cols] = df_train[feature_cols].fillna(-1)\n",
    "df_test[feature_cols] = df_test[feature_cols].fillna(-1)\n",
    "\n",
    "# Handle inf\n",
    "print(\"Handling infinite values...\")\n",
    "df_train = df_train.replace([np.inf, -np.inf], 999999)\n",
    "df_test = df_test.replace([np.inf, -np.inf], 999999)\n",
    "\n",
    "print(\"\\n‚úÖ Features cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving weighted features...\")\n",
    "\n",
    "df_train.to_parquet(os.path.join(output_dir, 'features_train_v3_weighted.parquet'), index=False)\n",
    "df_test.to_parquet(os.path.join(output_dir, 'features_test_v3_weighted.parquet'), index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to {output_dir}/\")\n",
    "print(f\"  - features_train_v3_weighted.parquet: {df_train.shape}\")\n",
    "print(f\"  - features_test_v3_weighted.parquet: {df_test.shape}\")\n",
    "\n",
    "# Save feature names\n",
    "with open(os.path.join(output_dir, 'feature_names_v3_weighted.txt'), 'w') as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "print(f\"\\n Feature names: {output_dir}/feature_names_v3_weighted.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEIGHTED FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "print(f\"  Train samples: {df_train.shape[0]:,}\")\n",
    "print(f\"  Test samples: {df_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY IMPROVEMENTS:\")\n",
    "print(f\"  1. Action weights: order (5.0), to_cart (3.0), favorite (2.5), search (2.0), click (1.0)\")\n",
    "print(f\"  2. Weighted aggregations: weighted_total_actions, recency_weighted, etc.\")\n",
    "print(f\"  3. High-intent features: high_intent_ratio, action_diversity\")\n",
    "print(f\"  4. Weighted periodic features: weighted_actions per period\")\n",
    "\n",
    "print(f\"\\nüìÅ New Features (examples):\")\n",
    "weighted_features = [col for col in feature_cols if 'weighted' in col]\n",
    "print(f\"  Weighted features ({len(weighted_features)}): {weighted_features[:10]}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Train models with weighted features\")\n",
    "print(f\"  2. Compare AUC with v2 (non-weighted)\")\n",
    "print(f\"  3. Analyze feature importance of weighted features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample\n",
    "print(\"\\nSample of weighted features:\")\n",
    "display_cols = ['user_id', 'target', 'weighted_total_actions', 'high_intent_ratio', \n",
    "                'action_diversity', 'recency_weighted_order', 'recency_weighted_search']\n",
    "display_cols = [c for c in display_cols if c in df_train.columns]\n",
    "df_train[display_cols].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
